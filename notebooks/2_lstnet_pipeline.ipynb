{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\0_amri_local\\11_aqi_forecast\\conda_aqi_forecast\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from tqdm.auto import tqdm\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from flexitext import flexitext\n",
    "# import seaborn as sns\n",
    "# import plotly.graph_objs as go\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import lightning.pytorch as pl\n",
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from helper_functions import epa_taiwan_data_pipeline, engine\n",
    "from models import lstnet_gokul, lstnet_laigoukun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed to 420\n",
    "pl.seed_everything(420)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = pathlib.Path(os.getcwd()).parent\n",
    "raw_data_dir = root_dir / \"data/0_raw\"\n",
    "processed_data_dir = root_dir / \"data/1_processed\"\n",
    "experiment_dir = root_dir / \"experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the data\n",
    "- Feature engineering\n",
    "- Turn the data into tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = 2018\n",
    "# site_name = \"Banqiao\"\n",
    "# columns = [\"SiteEngName\",\"PM2.5\",\"AMB_TEMP\",\"CH4\",'CO',\"NMHC\",\"read_time\"]\n",
    "\n",
    "# # import data\n",
    "# pm25_df = epa_taiwan_data_pipeline.import_epa_data(site_name=site_name, year=year)[columns]\n",
    "\n",
    "# # basic preprocessing\n",
    "# pm25_df = epa_taiwan_data_pipeline.standardize_df(pm25_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_split = 0.6\n",
    "\n",
    "# train_data = pm25_df.iloc[:int(len(pm25_df)*train_split),:]\n",
    "# print(f\"All data length:{len(pm25_df)} \\nTrain data length:{len(train_data)}\")\n",
    "# train_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_columns = ['pm2.5', 'amb_temp', 'ch4', 'co', 'nmhc']\n",
    "# normalized_column_names = []\n",
    "# for column in normalized_columns:\n",
    "#     normalized_column_name = column + '_normalized'\n",
    "#     normalized_column_names.append(normalized_column_name)\n",
    "#     train_data[normalized_column_name] = (train_data[column] - train_data[column].min()) / (train_data[column].max() - train_data[column].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # verify dataset instances\n",
    "# temp_train_dataset = epa_taiwan_data_pipeline.AqiDataset(train_data, history_len=48, col_names=[normalized_column_names[0]], device=None)\n",
    "# # temp_train_dataset = epa_taiwan_data_pipeline.AqiDataset(train_data, history_len=48, col_names=[normalized_column_names[0]], device=device)\n",
    "# print(len(temp_train_dataset))\n",
    "# x, y = temp_train_dataset[0]\n",
    "# print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train data_loader\n",
    "# temp_train_data_loader = DataLoader(temp_train_dataset, batch_size=4)\n",
    "# X, Y = next(iter(temp_train_data_loader))\n",
    "# print(X.shape, Y.shape)\n",
    "# print(X.is_cuda, Y.is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the dummy model\n",
    "- Initiate dummy model\n",
    "- Initiate loss and optimization function\n",
    "- Training process\n",
    "- Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the model\n",
    "# temp_model = lstnet_gokul.LSTNet(ar_window_size=24, num_features=1, recc1_out_channels=64, conv1_out_channels=32)#.to(device)\n",
    "# # next(temp_model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X, Y in temp_train_data_loader:\n",
    "#     print(X.shape)\n",
    "#     out = temp_model(X)\n",
    "#     print(Y.shape, out.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_len = 48\n",
    "# batch_size = 8\n",
    "\n",
    "# epochs = 10\n",
    "\n",
    "# lr = 1e-3\n",
    "# weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(temp_model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss_list = []\n",
    "\n",
    "# if os.path.exists(rf'testing_log\\temp_Running_Loss_{epochs}_epoch.txt'):\n",
    "#     os.remove(rf'testing_log\\temp_Running_Loss_{epochs}_epoch.txt')\n",
    "\n",
    "# if os.path.exists(rf'testing_log\\temp_Epoch_Loss_{epochs}_epoch.txt'):\n",
    "#     os.remove(rf'testing_log\\temp_Epoch_Loss_{epochs}_epoch.txt')\n",
    "        \n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     epoch_loss_train = 0\n",
    "#     for i, batch in enumerate(temp_train_data_loader, start=1):\n",
    "#         X, Y = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         Y_pred = temp_model(X)\n",
    "#         loss = criterion(Y_pred, Y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         with open(rf'testing_log\\temp_Running_Loss_{epochs}_epoch.txt', 'a+') as file:\n",
    "#             file.write(f'{loss.item()}\\n')\n",
    "#         epoch_loss_train += loss.item()\n",
    "\n",
    "#     epoch_loss_train = epoch_loss_train / len(temp_train_data_loader)\n",
    "#     train_loss_list.append(epoch_loss_train)\n",
    "\n",
    "#     with open(rf'testing_log\\temp_Epoch_Loss_{epochs}_epoch.txt', 'a+') as file:\n",
    "#         file.write(f'{epoch_loss_train}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = f\"test_LSTNet_uni-E{epochs}\"\n",
    "# model_path = rf\"saved_models\\{model_name}.pth\"\n",
    "\n",
    "# if not os.path.exists(model_path):\n",
    "#     torch.save(temp_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_loss = pd.read_table(rf'testing_log\\temp_Epoch_Loss_{epochs}_epoch.txt', header=None)\n",
    "# running_loss = pd.read_table(rf'testing_log\\temp_Running_Loss_{epochs}_epoch.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "# epoch_loss.plot(\n",
    "#     y = 0,\n",
    "#     label = \"epoch_loss\",\n",
    "#     ax=ax[0]\n",
    "# )\n",
    "\n",
    "# running_loss.plot(\n",
    "#     y = 0,\n",
    "#     label = \"running_loss\",\n",
    "#     ax=ax[1]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pm25_df.iloc[int(len(pm25_df)*train_split):,:]\n",
    "# print(f\"All data length:{len(pm25_df)} \\nTrain data length:{len(test_data)}\")\n",
    "# test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = epa_taiwan_data_pipeline.AqiDataset(test_data, history_len=history_len, col_names=[normalized_column_names[0]], device=None)\n",
    "# test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_df_norm(\n",
    "    df:pd.DataFrame,\n",
    "    target:str='pm2.5',\n",
    "    cols:List=['pm2.5', 'amb_temp', 'ch4', 'co', 'nmhc']\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"do a normalization to a dataframe\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): the dataframe to be normalized\n",
    "        target (str, optional): the target to be predicted later. Defaults to 'pm2.5'.\n",
    "        cols (List, optional): columns that will be normalized. Defaults to ['pm2.5', 'amb_temp', 'ch4', 'co', 'nmhc'].\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, float, float]: return the normalized df and min and max value of the target\n",
    "    \"\"\"\n",
    "    for column in cols:\n",
    "        normalized_column_name = column + '_normalized'\n",
    "        df[normalized_column_name] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())\n",
    "        # max_column_name = column + '_max'\n",
    "        # df[max_column_name] = df[column].max()\n",
    "        # min_column_name = column + '_min'\n",
    "        # df[min_column_name] = df[column].min()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = min_max_df_norm(test_data)\n",
    "# test_min_pm, test_max_pm = test_data[\"pm2.5\"].min(), test_data[\"pm2.5\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lstnet_gokul.LSTNet(7, 1, 64, 32)\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in tqdm(enumerate(test_data_loader, start=1),leave=False, total=len(test_data_loader)):\n",
    "#     X, Y = batch\n",
    "#     Y_pred = temp_model(X).detach().numpy()\n",
    "#     if i == 1:\n",
    "#         predictions = Y_pred\n",
    "#     else:\n",
    "#         predictions = np.concatenate((predictions, Y_pred), axis=0)\n",
    "\n",
    "# predictions = predictions * (test_max_pm - test_min_pm) + test_min_pm\n",
    "\n",
    "# columns = ['pm2.5']\n",
    "# predictions = pd.DataFrame(predictions, columns=columns)\n",
    "# predictions['read_time'] = test_data.reset_index()['read_time']\n",
    "# # print(predictions.shape)\n",
    "# # predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for length in [100, 24]:\n",
    "#     temp_train = train_data[['pm2.5', 'read_time']].reset_index().tail(length).reset_index().drop(columns=[\"index\", \"level_0\"]).rename(columns={'pm2.5':'pm2.5_train'})\n",
    "#     temp_test = test_data[['pm2.5', 'read_time']].reset_index().head(length).reset_index().drop(columns=[\"index\", \"level_0\"]).rename(columns={'pm2.5':'pm2.5_test'})\n",
    "#     temp_pred = predictions.head(length).rename(columns={'pm2.5':'pm2.5_prediction'})\n",
    "    \n",
    "#     temp_pred_test = pd.merge(temp_pred, temp_test, on='read_time', how='inner')\n",
    "    \n",
    "#     # Calculate MSE and RMSE\n",
    "#     temp_pred_test['squared_error'] = (temp_pred_test['pm2.5_test'] - temp_pred_test['pm2.5_prediction']) ** 2\n",
    "#     mse = temp_pred_test['squared_error'].mean()\n",
    "#     rmse = (temp_pred_test['squared_error'].mean())**.5\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize = (20,5))\n",
    "    \n",
    "#     ax.spines[\"right\"].set_visible(False)\n",
    "#     ax.spines[\"top\"].set_visible(False)\n",
    "#     plt.xlabel(\"\")\n",
    "    \n",
    "#     temp_train.plot(\n",
    "#         x=\"read_time\",\n",
    "#         y=\"pm2.5_train\",\n",
    "#         ax=ax,\n",
    "#         label=\"Train\",\n",
    "#         color=\"blue\",\n",
    "#         lw=2\n",
    "#         )\n",
    "\n",
    "#     temp_pred_test.plot(\n",
    "#         x=\"read_time\",\n",
    "#         y=\"pm2.5_prediction\",\n",
    "#         ax=ax,\n",
    "#         label=\"Prediction\",\n",
    "#         color=\"red\",\n",
    "#         marker=\"o\",\n",
    "#         lw=3\n",
    "#         )\n",
    "    \n",
    "#     # Define the confidence interval\n",
    "#     ci = 0.95 * (temp_pred_test['squared_error'].std() / np.sqrt(len(temp_pred_test)))\n",
    "    \n",
    "#     ax.fill_between(\n",
    "#         temp_pred_test.read_time.values, \n",
    "#         (temp_pred_test[\"pm2.5_prediction\"]-ci).to_numpy(), \n",
    "#         (temp_pred_test[\"pm2.5_prediction\"]+ci).to_numpy(), \n",
    "#         color='yellow', alpha=0.2,\n",
    "#         label=r\"95% confidence interval\"\n",
    "#         )\n",
    "    \n",
    "#     temp_pred_test.plot(\n",
    "#         x=\"read_time\",\n",
    "#         y=\"pm2.5_test\",\n",
    "#         linestyle='--',\n",
    "#         ax=ax,\n",
    "#         label=\"True Value\",\n",
    "#         color=\"black\",\n",
    "#         marker=\"o\",\n",
    "#         lw=2\n",
    "#         )\n",
    "    \n",
    "#     title = \"<name:monospace, size:18><weight:bold>Forecasting Result</></>\"\n",
    "#     flexitext(0, 1.20, title, va=\"top\", ax=ax)\n",
    "\n",
    "#     subtitle = (f\"<name:monospace, size:12, color:#454545>The AQI prediction value of the first {length} hours of data\\n<color: #d43535>RMSE: {rmse:.2f}</></>\")\n",
    "#     flexitext(0, 1.12, subtitle, va=\"top\", ax=ax)\n",
    "    \n",
    "#     # Shrink current axis's height by 10% on the bottom\n",
    "#     box = ax.get_position()\n",
    "#     ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "#                     box.width, box.height * 0.9])\n",
    "\n",
    "#     # Put a legend below current axis\n",
    "#     ax.legend(\n",
    "#         loc='upper center', \n",
    "#         bbox_to_anchor=(0.8, 1.15), # original (0.5, -0.05)\n",
    "#         fancybox=True, shadow=True, ncol=5\n",
    "#         )\n",
    "    \n",
    "#     plt.xlabel(None)\n",
    "    \n",
    "#     # plt.savefig(\n",
    "#     #     rf'images\\prediction_result_{length}_hour.png',\n",
    "#     #     bbox_inches='tight'\n",
    "#     #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helper_functions import engine\n",
    "# from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_writer(experiment_name: str, \n",
    "#                   model_name: str) -> SummaryWriter():\n",
    "#     \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
    "\n",
    "#     log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n",
    "\n",
    "#     Where timestamp is the current date in YYYY-MM-DD format.\n",
    "\n",
    "#     Args:\n",
    "#         experiment_name (str): Name of experiment.\n",
    "#         model_name (str): Name of model.\n",
    "#         extra (str, optional): Anything extra to add to the directory. Defaults to None.\n",
    "\n",
    "#     Returns:\n",
    "#         torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n",
    "\n",
    "#     Example usage:\n",
    "#         # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n",
    "#         writer = create_writer(experiment_name=\"data_10_percent\",\n",
    "#                                model_name=\"effnetb2\",\n",
    "#                                extra=\"5_epochs\")\n",
    "#         # The above is the same as:\n",
    "#         writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n",
    "#     \"\"\"\n",
    "#     from datetime import datetime\n",
    "#     import os\n",
    "\n",
    "#     # Get timestamp of current date (all experiments on certain day live in same folder)\n",
    "#     timestamp = datetime.now().strftime(\"%Y_%m_%d\") # returns current date in YYYY-MM-DD format\n",
    "#     log_dir = os.path.join(\"experiment\", timestamp, experiment_name, model_name)\n",
    "\n",
    "#     print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
    "#     return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = 2018\n",
    "# site_name = \"Banqiao\"\n",
    "# columns = [\"SiteEngName\",\"PM2.5\",\"AMB_TEMP\",\"CH4\",'CO',\"NMHC\",\"read_time\"]\n",
    "\n",
    "# # import data\n",
    "# pm25_df = epa_taiwan_data_pipeline.import_epa_data(site_name=site_name, year=year)[columns]\n",
    "\n",
    "# # basic preprocessing\n",
    "# pm25_df = epa_taiwan_data_pipeline.standardize_df(pm25_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_split = 0.6\n",
    "# history_len = 48\n",
    "# batch_size = 8\n",
    "\n",
    "# train_data = pm25_df.iloc[:int(len(pm25_df)*train_split),:]\n",
    "# print(f\"All data length:{len(pm25_df)} \\nTrain data length:{len(train_data)}\")\n",
    "\n",
    "# normalized_columns = ['pm2.5', 'amb_temp', 'ch4', 'co', 'nmhc']\n",
    "# normalized_column_names = []\n",
    "# for column in normalized_columns:\n",
    "#     normalized_column_name = column + '_normalized'\n",
    "#     normalized_column_names.append(normalized_column_name)\n",
    "#     train_data[normalized_column_name] = (train_data[column] - train_data[column].min()) / (train_data[column].max() - train_data[column].min())\n",
    "\n",
    "# temp_train_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "#     train_data, \n",
    "#     history_len=history_len, \n",
    "#     col_names=[normalized_column_names[0]], \n",
    "#     device=None)\n",
    "\n",
    "# # train data_loader\n",
    "# temp_train_data_loader = DataLoader(temp_train_dataset, batch_size=batch_size)\n",
    "# X, Y = next(iter(temp_train_data_loader))\n",
    "# print(X.shape, Y.shape)\n",
    "# print(X.is_cuda, Y.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pm25_df.iloc[int(len(pm25_df)*train_split):,:]\n",
    "# print(f\"All data length:{len(pm25_df)} \\nTrain data length:{len(test_data)}\")\n",
    "\n",
    "# for column in normalized_columns:\n",
    "#     normalized_column_name = column + '_normalized'\n",
    "#     normalized_column_names.append(normalized_column_name)\n",
    "#     test_data[normalized_column_name] = (test_data[column] - test_data[column].min()) / (test_data[column].max() - test_data[column].min())\n",
    "\n",
    "# test_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "#     test_data, \n",
    "#     history_len=history_len, \n",
    "#     col_names=[normalized_column_names[0]], \n",
    "#     device=None)\n",
    "# test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_model = lstnet_gokul.LSTNet(\n",
    "#     ar_window_size=24, \n",
    "#     num_features=1, \n",
    "#     recc1_out_channels=64, \n",
    "#     conv1_out_channels=32)#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 2\n",
    "\n",
    "# lr = 1e-3\n",
    "# weight_decay = 0.01\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(temp_model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(2):\n",
    "#     a,b = engine.train_step(\n",
    "#         temp_model,\n",
    "#         temp_train_data_loader,\n",
    "#         criterion,\n",
    "#         optimizer=optimizer,\n",
    "#         device='cpu'\n",
    "#     )\n",
    "#     print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine.train(\n",
    "#     model=temp_model,\n",
    "#     train_dataloader=temp_train_data_loader,\n",
    "#     test_dataloader=test_data_loader,\n",
    "#     optimizer=optimizer,\n",
    "#     loss_fn=criterion,\n",
    "#     epochs=epochs,\n",
    "#     device='cpu',\n",
    "#     # writer=None,\n",
    "#     writer=create_writer(\n",
    "#         experiment_name=\"test\",\n",
    "#         model_name=\"LSTNET\"\n",
    "#         )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the experiment name\n",
    "# timestamp = datetime.now().strftime(\"%Y_%m_%d\") # returns current date in YYYY-MM-DD format\n",
    "\n",
    "# try:\n",
    "#     mlflow.set_experiment(f\"{timestamp}\")\n",
    "# except:\n",
    "#     os.mkdir(\"mlruns\")\n",
    "#     mlflow.set_experiment(f\"{timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment name\n",
    "experiment_name = datetime.now().strftime(\"%Y_%m_%d\") # returns current date in YYYY-MM-DD format\n",
    "\n",
    "# Check if the experiment exists, and if not, create it\n",
    "if not mlflow.get_experiment_by_name(experiment_name):\n",
    "    mlflow.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLFLOW_TRACKING_URI=https://dagshub.com/amrirasyidi/air_quality_forecasting.mlflow \\\n",
    "# MLFLOW_TRACKING_USERNAME=amrirasyidi \\\n",
    "# MLFLOW_TRACKING_PASSWORD=a2c9e1ebaf6ce8285a9cced5e2c757c386254b7a \\\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'amrirasyidi'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = 'a2c9e1ebaf6ce8285a9cced5e2c757c386254b7a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7b2c4c7f584d69a26f1006bb4478f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTNET model (learning_rate=0.001000, batch_size=8.000000):\n",
      "  Epoch average training loss: [0.0059731285234266955]\n",
      "  Epoch average test loss: [0.0075304437235293635]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'test_model' already exists. Creating a new version of this model...\n",
      "2023/08/30 12:17:26 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: test_model, version 2\n",
      "Created version '2' of model 'test_model'.\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.6\n",
    "history_len = 48\n",
    "batch_size = 8\n",
    "\n",
    "with mlflow.start_run(experiment_id=mlflow.get_experiment_by_name(experiment_name).experiment_id):\n",
    "    # Load and preprocess your data\n",
    "    year = 2018\n",
    "    site_name = \"Banqiao\"\n",
    "    columns = [\"SiteEngName\",\"PM2.5\",\"AMB_TEMP\",\"CH4\",'CO',\"NMHC\",\"read_time\"]\n",
    "\n",
    "    # import data\n",
    "    pm25_df = epa_taiwan_data_pipeline.import_epa_data(site_name=site_name, year=year)[columns]\n",
    "    # basic preprocessing\n",
    "    pm25_df = epa_taiwan_data_pipeline.standardize_df(pm25_df)\n",
    "\n",
    "    train_data = pm25_df.iloc[:int(len(pm25_df)*train_split),:]\n",
    "    # print(f\"All data length:{len(pm25_df)} \\nTrain data length:{len(train_data)}\")\n",
    "\n",
    "    normalized_columns = ['pm2.5', 'amb_temp', 'ch4', 'co', 'nmhc']\n",
    "    normalized_column_names = [column + '_normalized' for column in normalized_columns]\n",
    "    \n",
    "    train_data = min_max_df_norm(train_data)\n",
    "\n",
    "    temp_train_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "        train_data,\n",
    "        history_len=history_len,\n",
    "        col_names=[normalized_column_names[0]],\n",
    "        device=None)\n",
    "\n",
    "    # train data_loader\n",
    "    temp_train_data_loader = DataLoader(temp_train_dataset, batch_size=batch_size)\n",
    "    \n",
    "    test_data = pm25_df.iloc[int(len(pm25_df)*train_split):,:]\n",
    "    # print(f\"All data length:{len(pm25_df)} \\nTrain data length:{len(test_data)}\")\n",
    "\n",
    "    for column in normalized_columns:\n",
    "        normalized_column_name = column + '_normalized'\n",
    "        normalized_column_names.append(normalized_column_name)\n",
    "        test_data[normalized_column_name] = (test_data[column] - test_data[column].min()) / (test_data[column].max() - test_data[column].min())\n",
    "\n",
    "    test_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "        test_data, \n",
    "        history_len=history_len, \n",
    "        col_names=[normalized_column_names[0]], \n",
    "        device=None)\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Create your PyTorch model\n",
    "    temp_model = lstnet_gokul.LSTNet(\n",
    "        ar_window_size=24, \n",
    "        num_features=1, \n",
    "        recc1_out_channels=64, \n",
    "        conv1_out_channels=32)#.to(device)\n",
    "    \n",
    "    epochs = 2\n",
    "    lr = 1e-3\n",
    "    weight_decay = 0.01\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(temp_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Define your training loop\n",
    "    epoch_avg_train_loss, epoch_avg_test_loss = engine.train(\n",
    "        model=temp_model,\n",
    "        train_dataloader=temp_train_data_loader,\n",
    "        test_dataloader=test_data_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=criterion,\n",
    "        epochs=epochs,\n",
    "        device='cpu',\n",
    "        writer=None,\n",
    "    )\n",
    "    \n",
    "    print(\"LSTNET model (learning_rate={:f}, batch_size={:f}):\".format(lr, batch_size))\n",
    "    print(\"  Epoch average training loss: %s\" % epoch_avg_train_loss)\n",
    "    print(\"  Epoch average test loss: %s\" % epoch_avg_test_loss)\n",
    "\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_params({\"learning_rate\": lr, \"batch_size\": batch_size})\n",
    "\n",
    "    # Log metrics during training\n",
    "    mlflow.log_metrics(\n",
    "        {\"train_loss\": epoch_avg_train_loss[0], \"test_loss\": epoch_avg_test_loss[0]},\n",
    "        # step=epoch\n",
    "    )\n",
    "\n",
    "    # # Log additional artifacts\n",
    "    # mlflow.log_artifact(\"path/to/your/training_plots.png\")\n",
    "    \n",
    "    ## For Remote server only (DAGShub)\n",
    "    \n",
    "    remote_server_uri=\"https://dagshub.com/amrirasyidi/air_quality_forecasting.mlflow\"\n",
    "    mlflow.set_tracking_uri(remote_server_uri)\n",
    "\n",
    "    tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "    \n",
    "    # Model registry does not work with file store\n",
    "    if tracking_url_type_store != \"file\":\n",
    "        # Register the model\n",
    "        # There are other ways to use the Model Registry, which depends on the use case,\n",
    "        # please refer to the doc for more information:\n",
    "        # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "        mlflow.pytorch.log_model(\n",
    "            temp_model, \"temp_model\", registered_model_name=\"test_model\"\n",
    "        )\n",
    "    else:\n",
    "        mlflow.pytorch.log_model(temp_model, \"temp_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare different scenarios\n",
    "    - [x] number of epochs --> [50, 100, 200]\n",
    "    - [x] lookback periods --> [24, 24x2, 24x7, 24x30] (history_len)\n",
    "    - [x] batch size --> [16, 64, 128]\n",
    "    - [x] loss function --> [MSE (nn.MSELoss()), MAE (nn.L1Loss()), Huber Loss (nn.SmoothL1Loss())]\n",
    "- Log the experiment\n",
    "- Monitor the result with MLFlow or ~~tensorboard~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [10, 20, 50]\n",
    "lookback_periods = [24//2, 24, 24*2, 24*7]\n",
    "batch_sizes = [16, 64, 128]\n",
    "# loss_functions = [nn.MSELoss(), nn.SmoothL1Loss()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = root_dir / \"experiment\"\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.mkdir(experiment_dir)\n",
    "    \n",
    "manual_exp_dir = experiment_dir / \"manual\"\n",
    "if not os.path.exists(manual_exp_dir):\n",
    "    os.mkdir(manual_exp_dir)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d\") # returns current date in YYYY-MM-DD format\n",
    "current_manual_exp_dir = manual_exp_dir / str(timestamp)\n",
    "if not os.path.exists(current_manual_exp_dir):\n",
    "    os.mkdir(current_manual_exp_dir)\n",
    "    \n",
    "lstnet_gokul_exp_dir = current_manual_exp_dir / \"LSTNET_UNI_GOKUL\"\n",
    "if not os.path.exists(lstnet_gokul_exp_dir):\n",
    "    os.mkdir(lstnet_gokul_exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss_tracker_name = \"running_loss.txt\"\n",
    "epoch_loss_tracker_name = \"epoch_loss.txt\"\n",
    "\n",
    "for epoch in epochs:\n",
    "    for batch_size in batch_sizes:\n",
    "        for lookback in lookback_periods:\n",
    "            model_name = f\"{epoch}E_{lookback}W_{batch_size}B\"\n",
    "\n",
    "            train_loss_list = []\n",
    "\n",
    "            if os.path.exists(lstnet_gokul_exp_dir / running_loss_tracker_name):\n",
    "                os.remove(lstnet_gokul_exp_dir / running_loss_tracker_name)\n",
    "\n",
    "            if os.path.exists(lstnet_gokul_exp_dir / epoch_loss_tracker_name):\n",
    "                os.remove(lstnet_gokul_exp_dir / epoch_loss_tracker_name)\n",
    "                    \n",
    "            for epoch in tqdm(range(epochs)):\n",
    "                epoch_loss_train = 0\n",
    "                for batch_no, (X, Y) in enumerate(temp_train_data_loader, start=1):\n",
    "                    X, Y = X.to(device), Y.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    Y_pred = temp_model(X)\n",
    "                    \n",
    "                    loss = criterion(Y_pred, Y)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "\n",
    "                    with open(lstnet_gokul_exp_dir / running_loss_tracker_name, 'a+') as file:\n",
    "                        file.write(f'{loss.item()}\\n')\n",
    "\n",
    "                    epoch_loss_train += loss.item()\n",
    "\n",
    "                epoch_loss_train = epoch_loss_train / len(temp_train_data_loader)\n",
    "                train_loss_list.append(epoch_loss_train)\n",
    "\n",
    "                with open(lstnet_gokul_exp_dir / epoch_loss_tracker_name, 'a+') as file:\n",
    "                    file.write(f'{epoch_loss_train}\\n')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_testing = lstnet_gokul.LSTNet(7, 1, 64, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the best model\n",
    "- Prepare the test data\n",
    "- Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
