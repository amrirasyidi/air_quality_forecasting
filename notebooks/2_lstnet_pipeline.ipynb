{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from tqdm.auto import tqdm\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from flexitext import flexitext\n",
    "# import seaborn as sns\n",
    "# import plotly.graph_objs as go\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning.pytorch as pl\n",
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from helper_functions import epa_taiwan_data_pipeline, engine\n",
    "from models import lstnet_gokul, lstnet_laigoukun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 420\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed to 420\n",
    "pl.seed_everything(420)\n",
    "\n",
    "device = \"cpu\"\n",
    "# device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = pathlib.Path(os.getcwd()).parent\n",
    "raw_data_dir = root_dir / \"data/0_raw\"\n",
    "processed_data_dir = root_dir / \"data/1_processed\"\n",
    "experiment_dir = root_dir / \"experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the data\n",
    "- Feature engineering\n",
    "- Turn the data into tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018\n",
    "site_name = \"Banqiao\"\n",
    "columns = [\"SiteEngName\",\"PM2.5\",\"AMB_TEMP\",\"CH4\",'CO',\"NMHC\",\"read_time\"]\n",
    "\n",
    "# import data\n",
    "pm25_df = epa_taiwan_data_pipeline.import_epa_data(site_name=site_name, year=year)[columns]\n",
    "\n",
    "# basic preprocessing\n",
    "pm25_df = epa_taiwan_data_pipeline.standardize_df(pm25_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.6\n",
    "history_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data length:8760 \n",
      "Train data length:5256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>siteengname</th>\n",
       "      <th>pm2.5</th>\n",
       "      <th>amb_temp</th>\n",
       "      <th>ch4</th>\n",
       "      <th>co</th>\n",
       "      <th>nmhc</th>\n",
       "      <th>read_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>399305</th>\n",
       "      <td>Banqiao</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2018-08-07 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399381</th>\n",
       "      <td>Banqiao</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2018-08-07 23:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       siteengname  pm2.5  amb_temp  ch4    co  nmhc           read_time\n",
       "399305     Banqiao    8.0      30.2  1.8  0.30  0.08 2018-08-07 22:00:00\n",
       "399381     Banqiao    9.0      29.9  1.8  0.23  0.06 2018-08-07 23:00:00"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pm25_df.iloc[:int(len(pm25_df)*train_split),:]\n",
    "print(f\"All data length:{len(pm25_df)} \\nTrain data length:{len(train_data)}\")\n",
    "train_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data length:8760 \n",
      "Test data length:3504\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>siteengname</th>\n",
       "      <th>pm2.5</th>\n",
       "      <th>amb_temp</th>\n",
       "      <th>ch4</th>\n",
       "      <th>co</th>\n",
       "      <th>nmhc</th>\n",
       "      <th>read_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>665609</th>\n",
       "      <td>Banqiao</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2018-12-31 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665685</th>\n",
       "      <td>Banqiao</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       siteengname  pm2.5  amb_temp  ch4    co  nmhc           read_time\n",
       "665609     Banqiao    4.0      16.6  1.9  0.34  0.08 2018-12-31 22:00:00\n",
       "665685     Banqiao    4.0      16.6  1.9  0.31  0.07 2018-12-31 23:00:00"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pm25_df.iloc[int(len(pm25_df)*train_split):,:]\n",
    "print(f\"All data length:{len(pm25_df)} \\nTest data length:{len(test_data)}\")\n",
    "test_data.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare different scenarios\n",
    "    - [x] number of epochs --> [50, 100, 200]\n",
    "    - [x] lookback periods --> [24, 24x2, 24x7, 24x30] (history_len)\n",
    "    - [x] batch size --> [16, 64, 128]\n",
    "    - [x] loss function --> [MSE (nn.MSELoss()), MAE (nn.L1Loss()), Huber Loss (nn.SmoothL1Loss())]\n",
    "- Log the experiment\n",
    "- Monitor the result with MLFlow or ~~tensorboard~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 0.01\n",
    "\n",
    "epochs = [10, 20, 50]\n",
    "lookback_periods = [24//2, 24, 24*2, 24*7]\n",
    "batch_sizes = [16, 32, 64]\n",
    "# loss_functions = [nn.MSELoss(), nn.SmoothL1Loss()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_dir = root_dir / \"experiment\"\n",
    "# if not os.path.exists(experiment_dir):\n",
    "#     os.mkdir(experiment_dir)\n",
    "    \n",
    "# manual_exp_dir = experiment_dir / \"manual\"\n",
    "# if not os.path.exists(manual_exp_dir):\n",
    "#     os.mkdir(manual_exp_dir)\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y_%m_%d\") # returns current date in YYYY-MM-DD format\n",
    "# current_manual_exp_dir = manual_exp_dir / str(timestamp)\n",
    "# if not os.path.exists(current_manual_exp_dir):\n",
    "#     os.mkdir(current_manual_exp_dir)\n",
    "    \n",
    "# lstnet_gokul_exp_dir = current_manual_exp_dir / \"LSTNET_UNI_GOKUL\"\n",
    "# if not os.path.exists(lstnet_gokul_exp_dir):\n",
    "#     os.mkdir(lstnet_gokul_exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running_loss_tracker_name = \"running_loss.txt\"\n",
    "# epoch_loss_tracker_name = \"epoch_loss.txt\"\n",
    "\n",
    "# for epoch in epochs:\n",
    "#     for batch_size in batch_sizes:\n",
    "#         for lookback in lookback_periods:\n",
    "#             model_name = f\"{epoch}E_{lookback}W_{batch_size}B\"\n",
    "\n",
    "#             train_loss_list = []\n",
    "\n",
    "#             if os.path.exists(lstnet_gokul_exp_dir / running_loss_tracker_name):\n",
    "#                 os.remove(lstnet_gokul_exp_dir / running_loss_tracker_name)\n",
    "\n",
    "#             if os.path.exists(lstnet_gokul_exp_dir / epoch_loss_tracker_name):\n",
    "#                 os.remove(lstnet_gokul_exp_dir / epoch_loss_tracker_name)\n",
    "                    \n",
    "#             for epoch in tqdm(range(epochs)):\n",
    "#                 epoch_loss_train = 0\n",
    "#                 for batch_no, (X, Y) in enumerate(temp_train_data_loader, start=1):\n",
    "#                     X, Y = X.to(device), Y.to(device)\n",
    "                    \n",
    "#                     optimizer.zero_grad()\n",
    "                    \n",
    "#                     Y_pred = temp_model(X)\n",
    "                    \n",
    "#                     loss = criterion(Y_pred, Y)\n",
    "#                     loss.backward()\n",
    "                    \n",
    "#                     optimizer.step()\n",
    "\n",
    "#                     with open(lstnet_gokul_exp_dir / running_loss_tracker_name, 'a+') as file:\n",
    "#                         file.write(f'{loss.item()}\\n')\n",
    "\n",
    "#                     epoch_loss_train += loss.item()\n",
    "\n",
    "#                 epoch_loss_train = epoch_loss_train / len(temp_train_data_loader)\n",
    "#                 train_loss_list.append(epoch_loss_train)\n",
    "\n",
    "#                 with open(lstnet_gokul_exp_dir / epoch_loss_tracker_name, 'a+') as file:\n",
    "#                     file.write(f'{epoch_loss_train}\\n')\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment name\n",
    "experiment_name = datetime.now().strftime(\"%Y_%m_%d\") # returns current date in YYYY-MM-DD format\n",
    "\n",
    "# Check if the experiment exists, and if not, create it\n",
    "if not mlflow.get_experiment_by_name(experiment_name):\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "    \n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'amrirasyidi'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = 'a2c9e1ebaf6ce8285a9cced5e2c757c386254b7a'\n",
    "\n",
    "normalized_columns = ['pm2.5', 'amb_temp', 'ch4', 'co', 'nmhc']\n",
    "combinations = [(epoch, batch_size, lookback) for epoch in epochs for batch_size in batch_sizes for lookback in lookback_periods]\n",
    "\n",
    "for epoch, batch_size, lookback in combinations:\n",
    "    # train data preprocessing\n",
    "    train_data, normalized_column_names = epa_taiwan_data_pipeline.min_max_df_norm(train_data)\n",
    "\n",
    "    train_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "        train_data,\n",
    "        history_len=history_len,\n",
    "        col_names=[normalized_column_names[0]], \n",
    "        device=device)\n",
    "\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "    # test data preprocessing\n",
    "    test_data, _ = epa_taiwan_data_pipeline.min_max_df_norm(test_data)\n",
    "\n",
    "    test_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "        train_data,\n",
    "        history_len=history_len,\n",
    "        col_names=[normalized_column_names[0]], \n",
    "        device=device)\n",
    "    \n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # model preparation\n",
    "    model_name = f\"{epoch}E_{lookback}W_{batch_size}B\"\n",
    "    model = lstnet_gokul.LSTNet(\n",
    "        ar_window_size=lookback,\n",
    "        num_features=1,\n",
    "        recc1_out_channels=64,\n",
    "        conv1_out_channels=32).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    with mlflow.start_run(experiment_id=mlflow.get_experiment_by_name(experiment_name).experiment_id):\n",
    "        # Define your training loop\n",
    "        epoch_avg_train_loss, epoch_avg_test_loss = engine.train(\n",
    "            model=model,\n",
    "            train_dataloader=train_data_loader,\n",
    "            test_dataloader=test_data_loader,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=criterion,\n",
    "            epochs=epoch,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # print(\"LSTNET model (learning_rate={:f}, batch_size={:f}):\".format(lr, batch_size))\n",
    "\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params({\n",
    "            # \"learning_rate\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epoch\": epoch,\n",
    "            \"lookback\": lookback\n",
    "            })\n",
    "\n",
    "        # Log metrics during training\n",
    "        mlflow.log_metrics(\n",
    "            {\n",
    "                \"train_loss\": epoch_avg_train_loss[0], \n",
    "                \"test_loss\": epoch_avg_test_loss[0]\n",
    "            },\n",
    "            step=epoch\n",
    "        )\n",
    "\n",
    "        # # Log additional artifacts\n",
    "        # mlflow.log_artifact(\"path/to/your/training_plots.png\")\n",
    "        \n",
    "        ## For Remote server only (DAGShub)\n",
    "        remote_server_uri=\"https://dagshub.com/amrirasyidi/air_quality_forecasting.mlflow\"\n",
    "        mlflow.set_tracking_uri(remote_server_uri)\n",
    "\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        # Model registry does not work with file store\n",
    "        if tracking_url_type_store != \"file\":\n",
    "            # Register the model\n",
    "            # There are other ways to use the Model Registry, which depends on the use case,\n",
    "            # please refer to the doc for more information:\n",
    "            # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "            mlflow.pytorch.log_model(\n",
    "                model, model_name, registered_model_name=model_name\n",
    "            )\n",
    "        else:\n",
    "            mlflow.pytorch.log_model(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the best model\n",
    "- Prepare the test data\n",
    "- Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
