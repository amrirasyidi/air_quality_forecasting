{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "import os\n",
    "import logging\n",
    "import pathlib\n",
    "# import glob\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from tqdm.auto import tqdm\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from flexitext import flexitext\n",
    "# import seaborn as sns\n",
    "# import plotly.graph_objs as go\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "# import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning.pytorch as pl\n",
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from helper_functions import epa_taiwan_data_pipeline, engine\n",
    "from models import lstnet_gokul, lstnet_laigoukun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 420\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed to 420\n",
    "pl.seed_everything(420)\n",
    "\n",
    "device = \"cpu\"\n",
    "# device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = pathlib.Path(os.getcwd()).parent\n",
    "raw_data_dir = root_dir / \"data/0_raw\"\n",
    "processed_data_dir = root_dir / \"data/1_processed\"\n",
    "experiment_dir = root_dir / \"experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the data\n",
    "- Feature engineering\n",
    "- Turn the data into tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018\n",
    "site_name = \"Banqiao\"\n",
    "columns = [\"SiteEngName\",\"PM2.5\",\"AMB_TEMP\",\"CH4\",'CO',\"NMHC\",\"read_time\"]\n",
    "\n",
    "# import data\n",
    "pm25_df = epa_taiwan_data_pipeline.import_epa_data(site_name=site_name, year=year)[columns]\n",
    "\n",
    "# basic preprocessing\n",
    "pm25_df = epa_taiwan_data_pipeline.standardize_df(pm25_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.6\n",
    "history_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data length:8760 \n",
      "Train data length:5256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>siteengname</th>\n",
       "      <th>pm2.5</th>\n",
       "      <th>amb_temp</th>\n",
       "      <th>ch4</th>\n",
       "      <th>co</th>\n",
       "      <th>nmhc</th>\n",
       "      <th>read_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>399305</th>\n",
       "      <td>Banqiao</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2018-08-07 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399381</th>\n",
       "      <td>Banqiao</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2018-08-07 23:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       siteengname  pm2.5  amb_temp  ch4    co  nmhc           read_time\n",
       "399305     Banqiao    8.0      30.2  1.8  0.30  0.08 2018-08-07 22:00:00\n",
       "399381     Banqiao    9.0      29.9  1.8  0.23  0.06 2018-08-07 23:00:00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pm25_df.iloc[:int(len(pm25_df)*train_split),:]\n",
    "print(f\"All data length:{len(pm25_df)} \\nTrain data length:{len(train_data)}\")\n",
    "train_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data length:8760 \n",
      "Test data length:3504\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>siteengname</th>\n",
       "      <th>pm2.5</th>\n",
       "      <th>amb_temp</th>\n",
       "      <th>ch4</th>\n",
       "      <th>co</th>\n",
       "      <th>nmhc</th>\n",
       "      <th>read_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>665609</th>\n",
       "      <td>Banqiao</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2018-12-31 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665685</th>\n",
       "      <td>Banqiao</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       siteengname  pm2.5  amb_temp  ch4    co  nmhc           read_time\n",
       "665609     Banqiao    4.0      16.6  1.9  0.34  0.08 2018-12-31 22:00:00\n",
       "665685     Banqiao    4.0      16.6  1.9  0.31  0.07 2018-12-31 23:00:00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pm25_df.iloc[int(len(pm25_df)*train_split):,:]\n",
    "print(f\"All data length:{len(pm25_df)} \\nTest data length:{len(test_data)}\")\n",
    "test_data.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare different scenarios\n",
    "    - [x] number of epochs --> [50, 100, 200]\n",
    "    - [x] lookback periods --> [24, 24x2, 24x7, 24x30] (history_len)\n",
    "    - [x] batch size --> [16, 64, 128]\n",
    "    - [x] loss function --> [MSE (nn.MSELoss()), MAE (nn.L1Loss()), Huber Loss (nn.SmoothL1Loss())]\n",
    "- Log the experiment\n",
    "- Monitor the result with MLFlow or ~~tensorboard~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 0.01\n",
    "\n",
    "epochs = [10, 20, 50]\n",
    "lookback_periods = [24//2, 24, 24*2, 24*7]\n",
    "batch_sizes = [16, 32, 64]\n",
    "# loss_functions = [nn.MSELoss(), nn.SmoothL1Loss()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_dir = root_dir / \"experiment\"\n",
    "# if not os.path.exists(experiment_dir):\n",
    "#     os.mkdir(experiment_dir)\n",
    "    \n",
    "# manual_exp_dir = experiment_dir / \"manual\"\n",
    "# if not os.path.exists(manual_exp_dir):\n",
    "#     os.mkdir(manual_exp_dir)\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y_%m_%d\") # returns current date in YYYY-MM-DD format\n",
    "# current_manual_exp_dir = manual_exp_dir / str(timestamp)\n",
    "# if not os.path.exists(current_manual_exp_dir):\n",
    "#     os.mkdir(current_manual_exp_dir)\n",
    "\n",
    "# # lstnet_gokul_exp_dir = current_manual_exp_dir / \"LSTNET_UNI_GOKUL\"\n",
    "# # if not os.path.exists(lstnet_gokul_exp_dir):\n",
    "# #     os.mkdir(lstnet_gokul_exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss_tracker_name = \"train_epoch_loss.txt\"\n",
    "# test_loss_tracker_name = \"test_epoch_loss.txt\"\n",
    "\n",
    "# combinations = [(epoch, batch_size, lookback) for epoch in epochs for batch_size in batch_sizes for lookback in lookback_periods]\n",
    "\n",
    "# for epoch, batch_size, lookback in combinations:\n",
    "# # for epoch, batch_size, lookback in [(2,4,12)]:\n",
    "#     # train data preprocessing\n",
    "#     train_data, normalized_column_names = epa_taiwan_data_pipeline.min_max_df_norm(train_data)\n",
    "\n",
    "#     train_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "#         train_data,\n",
    "#         history_len=history_len,\n",
    "#         col_names=[normalized_column_names[0]], \n",
    "#         device=device)\n",
    "\n",
    "#     train_data_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "#     # test data preprocessing\n",
    "#     test_data, _ = epa_taiwan_data_pipeline.min_max_df_norm(test_data)\n",
    "\n",
    "#     test_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "#         test_data,\n",
    "#         history_len=history_len,\n",
    "#         col_names=[normalized_column_names[0]], \n",
    "#         device=device)\n",
    "    \n",
    "#     test_data_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "#     # model preparation\n",
    "#     model_name = f\"{epoch}E_{lookback}W_{batch_size}B\"\n",
    "#     model = lstnet_gokul.LSTNet(\n",
    "#         ar_window_size=lookback,\n",
    "#         num_features=1,\n",
    "#         recc1_out_channels=64,\n",
    "#         conv1_out_channels=32).to(device)\n",
    "#     tracker_dir = current_manual_exp_dir / model_name\n",
    "#     if not os.path.exists(tracker_dir):\n",
    "#         os.mkdir(tracker_dir)\n",
    "\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     # Define your training loop\n",
    "#     epoch_avg_train_loss, epoch_avg_test_loss = engine.train(\n",
    "#         model=model,\n",
    "#         train_dataloader=train_data_loader,\n",
    "#         test_dataloader=test_data_loader,\n",
    "#         optimizer=optimizer,\n",
    "#         loss_fn=criterion,\n",
    "#         epochs=epoch,\n",
    "#         device=device,\n",
    "#         train_tracker_dir=str(tracker_dir / train_loss_tracker_name),\n",
    "#         test_tracker_dir=str(tracker_dir / test_loss_tracker_name)\n",
    "#     )\n",
    "\n",
    "#     # with open(tracker_dir / train_loss_tracker_name, 'a+') as file:\n",
    "#     #     file.write(f'{epoch_avg_train_loss}\\n')\n",
    "#     # with open(tracker_dir / test_loss_tracker_name, 'a+') as file:\n",
    "#     #     file.write(f'{epoch_avg_test_loss}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracked_files = list((current_manual_exp_dir / os.listdir(current_manual_exp_dir)[0]).glob(\"*.txt\"))\n",
    "\n",
    "# train_loss = pd.read_csv(tracked_files[1], delimiter = \"\\t\", header=None)\n",
    "# test_loss = pd.read_csv(tracked_files[0], delimiter = \"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# train_loss.plot(ax=ax, label=\"training\")\n",
    "# test_loss.plot(ax=ax, label=\"testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import engine_1, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28244e649a59477082605577b49dae93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LSTNET_20E_8B_12W' already exists. Creating a new version of this model...\n",
      "2023/09/02 19:01:28 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: LSTNET_20E_8B_12W, version 2\n",
      "Created version '2' of model 'LSTNET_20E_8B_12W'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model to: saved_models\\LSTNET_20E_8B_12W.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b06e600ab44c51bea9f2ab878b07cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'LSTNET_1E_8B_12W'.\n",
      "2023/09/02 19:02:54 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: LSTNET_1E_8B_12W, version 1\n",
      "Created version '1' of model 'LSTNET_1E_8B_12W'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model to: saved_models\\LSTNET_1E_8B_12W.pth\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    filename='LSTNET_MLFLOW_training.log',\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s:%(message)s\",\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# experiment_name = datetime.now().strftime(\"%Y_%m_%d\") # returns current date in YYYY-MM-DD format\n",
    "experiment_name = \"mlflow_lstnet_trial\"\n",
    "\n",
    "# Check if the experiment exists, and if not, create it\n",
    "if not mlflow.get_experiment_by_name(experiment_name):\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'amrirasyidi'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = 'a2c9e1ebaf6ce8285a9cced5e2c757c386254b7a'\n",
    "\n",
    "remote_server_uri=\"https://dagshub.com/amrirasyidi/air_quality_forecasting.mlflow\"\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "for epoch, batch_size, lookback in [(20,8,12),(1,8,12)]:\n",
    "    with mlflow.start_run(experiment_id=experiment_id) as run:\n",
    "        mlflow.log_param(\"Epoch\",epoch)\n",
    "        mlflow.log_param(\"Batch Size\",batch_size)\n",
    "        mlflow.log_param(\"Lookback Period\",lookback)\n",
    "\n",
    "        model_name = f\"LSTNET_{epoch}E_{batch_size}B_{lookback}W\"\n",
    "        model = lstnet_gokul.LSTNet(\n",
    "            ar_window_size=lookback,\n",
    "            num_features=1,\n",
    "            recc1_out_channels=64,\n",
    "            conv1_out_channels=32\n",
    "        )\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # train data preprocessing\n",
    "        train_data, normalized_column_names = epa_taiwan_data_pipeline.min_max_df_norm(train_data)\n",
    "\n",
    "        train_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "            train_data,\n",
    "            history_len=history_len,\n",
    "            col_names=[normalized_column_names[0]],\n",
    "            device=device)\n",
    "\n",
    "        train_data_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "        # test data preprocessing\n",
    "        test_data, _ = epa_taiwan_data_pipeline.min_max_df_norm(test_data)\n",
    "\n",
    "        test_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "            test_data,\n",
    "            history_len=history_len,\n",
    "            col_names=[normalized_column_names[0]], \n",
    "            device=device)\n",
    "\n",
    "        test_data_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        engine_1.train(\n",
    "            model=model,\n",
    "            model_name=model_name,\n",
    "            train_dataloader=train_data_loader,\n",
    "            test_dataloader=test_data_loader,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=criterion,\n",
    "            device=device,\n",
    "            epochs=epoch,\n",
    "            patience=3\n",
    "        )\n",
    "\n",
    "        # Model registry does not work with file store\n",
    "        if tracking_url_type_store != \"file\":\n",
    "            # Register the model\n",
    "            mlflow.pytorch.log_model(\n",
    "                model, model_name, registered_model_name=model_name\n",
    "            )\n",
    "        else:\n",
    "            mlflow.pytorch.log_model(model, model_name)\n",
    "\n",
    "        # save the model\n",
    "        utils.save_model(\n",
    "            model=model,\n",
    "            target_dir=\"saved_models\",\n",
    "            model_name=model_name + \".pth\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the experiment name\n",
    "# experiment_name = datetime.now().strftime(\"%Y_%m_%d\") # returns current date in YYYY-MM-DD format\n",
    "\n",
    "# # Check if the experiment exists, and if not, create it\n",
    "# if not mlflow.get_experiment_by_name(experiment_name):\n",
    "#     mlflow.create_experiment(experiment_name)\n",
    "\n",
    "# os.environ['MLFLOW_TRACKING_USERNAME'] = 'amrirasyidi'\n",
    "# os.environ['MLFLOW_TRACKING_PASSWORD'] = 'a2c9e1ebaf6ce8285a9cced5e2c757c386254b7a'\n",
    "\n",
    "# normalized_columns = ['pm2.5', 'amb_temp', 'ch4', 'co', 'nmhc']\n",
    "# combinations = [(epoch, batch_size, lookback) for epoch in epochs for batch_size in batch_sizes for lookback in lookback_periods]\n",
    "\n",
    "# mlflow.end_run()\n",
    "# # # with mlflow.start_run(experiment_id=mlflow.get_experiment_by_name(experiment_name).experiment_id) as run:\n",
    "# # with mlflow.start_run() as run:\n",
    "# #     ## For Remote server only (DAGShub)\n",
    "# #     remote_server_uri=\"https://dagshub.com/amrirasyidi/air_quality_forecasting.mlflow\"\n",
    "# #     mlflow.set_tracking_uri(remote_server_uri)\n",
    "# #     tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "# #     # for epoch, batch_size, lookback in combinations:\n",
    "# #     for epoch, batch_size, lookback in [(2,4,12)]:\n",
    "# #         # train data preprocessing\n",
    "# #         train_data, normalized_column_names = epa_taiwan_data_pipeline.min_max_df_norm(train_data)\n",
    "\n",
    "# #         train_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "# #             train_data,\n",
    "# #             history_len=history_len,\n",
    "# #             col_names=[normalized_column_names[0]], \n",
    "# #             device=device)\n",
    "\n",
    "# #         train_data_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "# #         # test data preprocessing\n",
    "# #         test_data, _ = epa_taiwan_data_pipeline.min_max_df_norm(test_data)\n",
    "\n",
    "# #         test_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "# #             test_data,\n",
    "# #             history_len=history_len,\n",
    "# #             col_names=[normalized_column_names[0]], \n",
    "# #             device=device)\n",
    "\n",
    "# #         test_data_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# #         # model preparation\n",
    "# #         model_name = f\"{epoch}E_{lookback}W_{batch_size}B\"\n",
    "# #         model = lstnet_gokul.LSTNet(\n",
    "# #             ar_window_size=lookback,\n",
    "# #             num_features=1,\n",
    "# #             recc1_out_channels=64,\n",
    "# #             conv1_out_channels=32).to(device)\n",
    "\n",
    "# #         criterion = nn.MSELoss()\n",
    "# #         optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# #         # Define your training loop\n",
    "# #         epoch_avg_train_loss, epoch_avg_test_loss = engine.train(\n",
    "# #             model=model,\n",
    "# #             train_dataloader=train_data_loader,\n",
    "# #             test_dataloader=test_data_loader,\n",
    "# #             optimizer=optimizer,\n",
    "# #             loss_fn=criterion,\n",
    "# #             epochs=epoch,\n",
    "# #             device=device,\n",
    "# #             train_tracker_dir=None,\n",
    "# #             test_tracker_dir=None\n",
    "# #         )\n",
    "\n",
    "# #         # print(\"LSTNET model (learning_rate={:f}, batch_size={:f}):\".format(lr, batch_size))\n",
    "\n",
    "# #         # Log hyperparameters\n",
    "# #         mlflow.log_param(\"epoch\",epoch)\n",
    "# #         mlflow.log_param(\"batch_size\",batch_size)\n",
    "# #         mlflow.log_param(\"lookback\",lookback)\n",
    "\n",
    "# #         # Log metrics during training\n",
    "# #         mlflow.log_metrics(\n",
    "# #             {\n",
    "# #                 \"train_loss\": epoch_avg_train_loss[0],\n",
    "# #                 \"test_loss\": epoch_avg_test_loss[0]\n",
    "# #             },\n",
    "# #             step=epoch\n",
    "# #         )\n",
    "\n",
    "# #         # # Log additional artifacts\n",
    "# #         # mlflow.log_artifact(\"path/to/your/training_plots.png\")\n",
    "\n",
    "# #         # # Model registry does not work with file store\n",
    "# #         # if tracking_url_type_store != \"file\":\n",
    "# #         #     # Register the model\n",
    "# #         #     # There are other ways to use the Model Registry, which depends on the use case,\n",
    "# #         #     # please refer to the doc for more information:\n",
    "# #         #     # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "# #         #     mlflow.pytorch.log_model(\n",
    "# #         #         model, model_name, registered_model_name=model_name\n",
    "# #         #     )\n",
    "# #         # else:\n",
    "# #         #     mlflow.pytorch.log_model(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the best model\n",
    "- Prepare the test data\n",
    "- Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
