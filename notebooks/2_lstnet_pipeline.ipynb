{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\0_amri_local\\11_aqi_forecast\\conda_aqi_forecast\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from tqdm.auto import tqdm\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from flexitext import flexitext\n",
    "# import seaborn as sns\n",
    "# import plotly.graph_objs as go\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning.pytorch as pl\n",
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from helper_functions import epa_taiwan_data_pipeline, engine\n",
    "from models import lstnet_gokul, lstnet_laigoukun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 420\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed to 420\n",
    "pl.seed_everything(420)\n",
    "\n",
    "device = \"cpu\"\n",
    "# device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = pathlib.Path(os.getcwd()).parent\n",
    "raw_data_dir = root_dir / \"data/0_raw\"\n",
    "processed_data_dir = root_dir / \"data/1_processed\"\n",
    "experiment_dir = root_dir / \"experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the data\n",
    "- Feature engineering\n",
    "- Turn the data into tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018\n",
    "site_name = \"Banqiao\"\n",
    "columns = [\"SiteEngName\",\"PM2.5\",\"AMB_TEMP\",\"CH4\",'CO',\"NMHC\",\"read_time\"]\n",
    "\n",
    "# import data\n",
    "pm25_df = epa_taiwan_data_pipeline.import_epa_data(site_name=site_name, year=year)[columns]\n",
    "\n",
    "# basic preprocessing\n",
    "pm25_df = epa_taiwan_data_pipeline.standardize_df(pm25_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.6\n",
    "history_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data length:8760 \n",
      "Train data length:5256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>siteengname</th>\n",
       "      <th>pm2.5</th>\n",
       "      <th>amb_temp</th>\n",
       "      <th>ch4</th>\n",
       "      <th>co</th>\n",
       "      <th>nmhc</th>\n",
       "      <th>read_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>399305</th>\n",
       "      <td>Banqiao</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2018-08-07 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399381</th>\n",
       "      <td>Banqiao</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2018-08-07 23:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       siteengname  pm2.5  amb_temp  ch4    co  nmhc           read_time\n",
       "399305     Banqiao    8.0      30.2  1.8  0.30  0.08 2018-08-07 22:00:00\n",
       "399381     Banqiao    9.0      29.9  1.8  0.23  0.06 2018-08-07 23:00:00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pm25_df.iloc[:int(len(pm25_df)*train_split),:]\n",
    "print(f\"All data length:{len(pm25_df)} \\nTrain data length:{len(train_data)}\")\n",
    "train_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data length:8760 \n",
      "Test data length:3504\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>siteengname</th>\n",
       "      <th>pm2.5</th>\n",
       "      <th>amb_temp</th>\n",
       "      <th>ch4</th>\n",
       "      <th>co</th>\n",
       "      <th>nmhc</th>\n",
       "      <th>read_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>665609</th>\n",
       "      <td>Banqiao</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2018-12-31 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665685</th>\n",
       "      <td>Banqiao</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       siteengname  pm2.5  amb_temp  ch4    co  nmhc           read_time\n",
       "665609     Banqiao    4.0      16.6  1.9  0.34  0.08 2018-12-31 22:00:00\n",
       "665685     Banqiao    4.0      16.6  1.9  0.31  0.07 2018-12-31 23:00:00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pm25_df.iloc[int(len(pm25_df)*train_split):,:]\n",
    "print(f\"All data length:{len(pm25_df)} \\nTest data length:{len(test_data)}\")\n",
    "test_data.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare different scenarios\n",
    "    - [x] number of epochs --> [50, 100, 200]\n",
    "    - [x] lookback periods --> [24, 24x2, 24x7, 24x30] (history_len)\n",
    "    - [x] batch size --> [16, 64, 128]\n",
    "    - [x] loss function --> [MSE (nn.MSELoss()), MAE (nn.L1Loss()), Huber Loss (nn.SmoothL1Loss())]\n",
    "- Log the experiment\n",
    "- Monitor the result with MLFlow or ~~tensorboard~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 0.01\n",
    "\n",
    "epochs = [10, 20, 50]\n",
    "lookback_periods = [24//2, 24, 24*2, 24*7]\n",
    "batch_sizes = [16, 32, 64]\n",
    "# loss_functions = [nn.MSELoss(), nn.SmoothL1Loss()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = root_dir / \"experiment\"\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.mkdir(experiment_dir)\n",
    "    \n",
    "manual_exp_dir = experiment_dir / \"manual\"\n",
    "if not os.path.exists(manual_exp_dir):\n",
    "    os.mkdir(manual_exp_dir)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d\") # returns current date in YYYY-MM-DD format\n",
    "current_manual_exp_dir = manual_exp_dir / str(timestamp)\n",
    "if not os.path.exists(current_manual_exp_dir):\n",
    "    os.mkdir(current_manual_exp_dir)\n",
    "\n",
    "# lstnet_gokul_exp_dir = current_manual_exp_dir / \"LSTNET_UNI_GOKUL\"\n",
    "# if not os.path.exists(lstnet_gokul_exp_dir):\n",
    "#     os.mkdir(lstnet_gokul_exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d626451ceb0146bf86b4e922694a9311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.0176 | test_loss: 0.0257 | \n",
      "Epoch: 2 | train_loss: 0.0056 | test_loss: 0.0104 | \n"
     ]
    }
   ],
   "source": [
    "train_loss_tracker_name = \"train_epoch_loss.txt\"\n",
    "test_loss_tracker_name = \"test_epoch_loss.txt\"\n",
    "\n",
    "combinations = [(epoch, batch_size, lookback) for epoch in epochs for batch_size in batch_sizes for lookback in lookback_periods]\n",
    "\n",
    "for epoch, batch_size, lookback in combinations:\n",
    "# for epoch, batch_size, lookback in [(2,4,12)]:\n",
    "    # train data preprocessing\n",
    "    train_data, normalized_column_names = epa_taiwan_data_pipeline.min_max_df_norm(train_data)\n",
    "\n",
    "    train_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "        train_data,\n",
    "        history_len=history_len,\n",
    "        col_names=[normalized_column_names[0]], \n",
    "        device=device)\n",
    "\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "    # test data preprocessing\n",
    "    test_data, _ = epa_taiwan_data_pipeline.min_max_df_norm(test_data)\n",
    "\n",
    "    test_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "        test_data,\n",
    "        history_len=history_len,\n",
    "        col_names=[normalized_column_names[0]], \n",
    "        device=device)\n",
    "    \n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # model preparation\n",
    "    model_name = f\"{epoch}E_{lookback}W_{batch_size}B\"\n",
    "    model = lstnet_gokul.LSTNet(\n",
    "        ar_window_size=lookback,\n",
    "        num_features=1,\n",
    "        recc1_out_channels=64,\n",
    "        conv1_out_channels=32).to(device)\n",
    "    tracker_dir = current_manual_exp_dir / model_name\n",
    "    if not os.path.exists(tracker_dir):\n",
    "        os.mkdir(tracker_dir)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # Define your training loop\n",
    "    epoch_avg_train_loss, epoch_avg_test_loss = engine.train(\n",
    "        model=model,\n",
    "        train_dataloader=train_data_loader,\n",
    "        test_dataloader=test_data_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=criterion,\n",
    "        epochs=epoch,\n",
    "        device=device,\n",
    "        train_tracker_dir=str(tracker_dir / train_loss_tracker_name),\n",
    "        test_tracker_dir=str(tracker_dir / test_loss_tracker_name)\n",
    "    )\n",
    "\n",
    "    # with open(tracker_dir / train_loss_tracker_name, 'a+') as file:\n",
    "    #     file.write(f'{epoch_avg_train_loss}\\n')\n",
    "    # with open(tracker_dir / test_loss_tracker_name, 'a+') as file:\n",
    "    #     file.write(f'{epoch_avg_test_loss}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracked_files = list((current_manual_exp_dir / os.listdir(current_manual_exp_dir)[0]).glob(\"*.txt\"))\n",
    "\n",
    "# train_loss = pd.read_csv(tracked_files[1], delimiter = \"\\t\", header=None)\n",
    "# test_loss = pd.read_csv(tracked_files[0], delimiter = \"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# train_loss.plot(ax=ax, label=\"training\")\n",
    "# test_loss.plot(ax=ax, label=\"testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f094942ce4484dbaba1e0ea359bd7cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.0380 | test_loss: 0.0330 | \n",
      "Epoch: 2 | train_loss: 0.0177 | test_loss: 0.0242 | \n",
      "Epoch: 3 | train_loss: 0.0125 | test_loss: 0.0179 | \n",
      "Epoch: 4 | train_loss: 0.0092 | test_loss: 0.0138 | \n",
      "Epoch: 5 | train_loss: 0.0072 | test_loss: 0.0112 | \n",
      "Epoch: 6 | train_loss: 0.0060 | test_loss: 0.0094 | \n",
      "Epoch: 7 | train_loss: 0.0052 | test_loss: 0.0083 | \n",
      "Epoch: 8 | train_loss: 0.0046 | test_loss: 0.0075 | \n",
      "Epoch: 9 | train_loss: 0.0042 | test_loss: 0.0069 | \n",
      "Epoch: 10 | train_loss: 0.0039 | test_loss: 0.0065 | \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb76ce47e3ba4a5d9021d0d6615ff9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.0129 | test_loss: 0.0089 | \n",
      "Epoch: 2 | train_loss: 0.0059 | test_loss: 0.0067 | \n",
      "Epoch: 3 | train_loss: 0.0046 | test_loss: 0.0061 | \n",
      "Epoch: 4 | train_loss: 0.0040 | test_loss: 0.0061 | \n",
      "Epoch: 5 | train_loss: 0.0037 | test_loss: 0.0058 | \n",
      "Epoch: 6 | train_loss: 0.0036 | test_loss: 0.0058 | \n",
      "Epoch: 7 | train_loss: 0.0035 | test_loss: 0.0059 | \n",
      "Epoch: 8 | train_loss: 0.0034 | test_loss: 0.0058 | \n",
      "Epoch: 9 | train_loss: 0.0033 | test_loss: 0.0057 | \n",
      "Epoch: 10 | train_loss: 0.0033 | test_loss: 0.0058 | \n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "INVALID_PARAMETER_VALUE: Response: {'error_code': 'INVALID_PARAMETER_VALUE'}\n\nThe cause of this error is typically due to repeated calls\nto an individual run_id event logging.\n\nIncorrect Example:\n---------------------------------------\nwith mlflow.start_run():\n    mlflow.log_param(\"depth\", 3)\n    mlflow.log_param(\"depth\", 5)\n---------------------------------------\n\nWhich will throw an MlflowException for overwriting a\nlogged parameter.\n\nCorrect Example:\n---------------------------------------\nwith mlflow.start_run():\n    with mlflow.start_run(nested=True):\n        mlflow.log_param(\"depth\", 3)\n    with mlflow.start_run(nested=True):\n        mlflow.log_param(\"depth\", 5)\n---------------------------------------\n\nWhich will create a new nested run for each individual\nmodel and prevent parameter key collisions within the\ntracking store.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRestException\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\0_amri_local\\11_aqi_forecast\\conda_aqi_forecast\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:297\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_param\u001b[1;34m(self, run_id, key, value)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mlog_param(run_id, param)\n\u001b[0;32m    298\u001b[0m \u001b[39mexcept\u001b[39;00m MlflowException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\0_amri_local\\11_aqi_forecast\\conda_aqi_forecast\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:211\u001b[0m, in \u001b[0;36mRestStore.log_param\u001b[1;34m(self, run_id, param)\u001b[0m\n\u001b[0;32m    208\u001b[0m req_body \u001b[39m=\u001b[39m message_to_json(\n\u001b[0;32m    209\u001b[0m     LogParam(run_uuid\u001b[39m=\u001b[39mrun_id, run_id\u001b[39m=\u001b[39mrun_id, key\u001b[39m=\u001b[39mparam\u001b[39m.\u001b[39mkey, value\u001b[39m=\u001b[39mparam\u001b[39m.\u001b[39mvalue)\n\u001b[0;32m    210\u001b[0m )\n\u001b[1;32m--> 211\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_endpoint(LogParam, req_body)\n",
      "File \u001b[1;32md:\\0_amri_local\\11_aqi_forecast\\conda_aqi_forecast\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:59\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[1;34m(self, api, json_body)\u001b[0m\n\u001b[0;32m     58\u001b[0m response_proto \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mResponse()\n\u001b[1;32m---> 59\u001b[0m \u001b[39mreturn\u001b[39;00m call_endpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[1;32md:\\0_amri_local\\11_aqi_forecast\\conda_aqi_forecast\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:203\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[1;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[0;32m    202\u001b[0m     response \u001b[39m=\u001b[39m http_request(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcall_kwargs)\n\u001b[1;32m--> 203\u001b[0m response \u001b[39m=\u001b[39m verify_rest_response(response, endpoint)\n\u001b[0;32m    204\u001b[0m js_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext)\n",
      "File \u001b[1;32md:\\0_amri_local\\11_aqi_forecast\\conda_aqi_forecast\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:135\u001b[0m, in \u001b[0;36mverify_rest_response\u001b[1;34m(response, endpoint)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mif\u001b[39;00m _can_parse_as_json_object(response\u001b[39m.\u001b[39mtext):\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mraise\u001b[39;00m RestException(json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext))\n\u001b[0;32m    136\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mRestException\u001b[0m: INVALID_PARAMETER_VALUE: Response: {'error_code': 'INVALID_PARAMETER_VALUE'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m mlflow\u001b[39m.\u001b[39mlog_param(\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,epoch)\n\u001b[0;32m     69\u001b[0m mlflow\u001b[39m.\u001b[39mlog_param(\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m,batch_size)\n\u001b[1;32m---> 70\u001b[0m mlflow\u001b[39m.\u001b[39;49mlog_param(\u001b[39m\"\u001b[39;49m\u001b[39mlookback\u001b[39;49m\u001b[39m\"\u001b[39;49m,lookback)\n\u001b[0;32m     72\u001b[0m \u001b[39m# Log metrics during training\u001b[39;00m\n\u001b[0;32m     73\u001b[0m mlflow\u001b[39m.\u001b[39mlog_metrics(\n\u001b[0;32m     74\u001b[0m     {\n\u001b[0;32m     75\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m: epoch_avg_train_loss[\u001b[39m0\u001b[39m], \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     step\u001b[39m=\u001b[39mepoch\n\u001b[0;32m     79\u001b[0m )\n",
      "File \u001b[1;32md:\\0_amri_local\\11_aqi_forecast\\conda_aqi_forecast\\Lib\\site-packages\\mlflow\\tracking\\fluent.py:601\u001b[0m, in \u001b[0;36mlog_param\u001b[1;34m(key, value)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[39mLog a parameter (e.g. model hyperparameter) under the current run. If no run is active,\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \u001b[39mthis method will create a new active run.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39m        assert value == 0.01\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    600\u001b[0m run_id \u001b[39m=\u001b[39m _get_or_start_run()\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\n\u001b[1;32m--> 601\u001b[0m \u001b[39mreturn\u001b[39;00m MlflowClient()\u001b[39m.\u001b[39;49mlog_param(run_id, key, value)\n",
      "File \u001b[1;32md:\\0_amri_local\\11_aqi_forecast\\conda_aqi_forecast\\Lib\\site-packages\\mlflow\\tracking\\client.py:806\u001b[0m, in \u001b[0;36mMlflowClient.log_param\u001b[1;34m(self, run_id, key, value)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_param\u001b[39m(\u001b[39mself\u001b[39m, run_id: \u001b[39mstr\u001b[39m, key: \u001b[39mstr\u001b[39m, value: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    752\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[39m    Log a parameter (e.g. model hyperparameter) against the run ID.\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[39m        status: FINISHED\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 806\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mlog_param(run_id, key, value)\n\u001b[0;32m    807\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[1;32md:\\0_amri_local\\11_aqi_forecast\\conda_aqi_forecast\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:301\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_param\u001b[1;34m(self, run_id, key, value)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merror_code \u001b[39m==\u001b[39m ErrorCode\u001b[39m.\u001b[39mName(INVALID_PARAMETER_VALUE):\n\u001b[0;32m    300\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m.\u001b[39mmessage\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mPARAM_VALIDATION_MSG\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 301\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(msg, INVALID_PARAMETER_VALUE)\n\u001b[0;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;31mMlflowException\u001b[0m: INVALID_PARAMETER_VALUE: Response: {'error_code': 'INVALID_PARAMETER_VALUE'}\n\nThe cause of this error is typically due to repeated calls\nto an individual run_id event logging.\n\nIncorrect Example:\n---------------------------------------\nwith mlflow.start_run():\n    mlflow.log_param(\"depth\", 3)\n    mlflow.log_param(\"depth\", 5)\n---------------------------------------\n\nWhich will throw an MlflowException for overwriting a\nlogged parameter.\n\nCorrect Example:\n---------------------------------------\nwith mlflow.start_run():\n    with mlflow.start_run(nested=True):\n        mlflow.log_param(\"depth\", 3)\n    with mlflow.start_run(nested=True):\n        mlflow.log_param(\"depth\", 5)\n---------------------------------------\n\nWhich will create a new nested run for each individual\nmodel and prevent parameter key collisions within the\ntracking store."
     ]
    }
   ],
   "source": [
    "# # Set the experiment name\n",
    "# experiment_name = datetime.now().strftime(\"%Y_%m_%d\") # returns current date in YYYY-MM-DD format\n",
    "\n",
    "# # Check if the experiment exists, and if not, create it\n",
    "# if not mlflow.get_experiment_by_name(experiment_name):\n",
    "#     mlflow.create_experiment(experiment_name)\n",
    "\n",
    "# os.environ['MLFLOW_TRACKING_USERNAME'] = 'amrirasyidi'\n",
    "# os.environ['MLFLOW_TRACKING_PASSWORD'] = 'a2c9e1ebaf6ce8285a9cced5e2c757c386254b7a'\n",
    "\n",
    "# normalized_columns = ['pm2.5', 'amb_temp', 'ch4', 'co', 'nmhc']\n",
    "# combinations = [(epoch, batch_size, lookback) for epoch in epochs for batch_size in batch_sizes for lookback in lookback_periods]\n",
    "\n",
    "# mlflow.end_run()\n",
    "# with mlflow.start_run(experiment_id=mlflow.get_experiment_by_name(experiment_name).experiment_id) as run:\n",
    "#     ## For Remote server only (DAGShub)\n",
    "#     remote_server_uri=\"https://dagshub.com/amrirasyidi/air_quality_forecasting.mlflow\"\n",
    "#     mlflow.set_tracking_uri(remote_server_uri)\n",
    "#     tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "#     for epoch, batch_size, lookback in combinations:\n",
    "#         # train data preprocessing\n",
    "#         train_data, normalized_column_names = epa_taiwan_data_pipeline.min_max_df_norm(train_data)\n",
    "\n",
    "#         train_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "#             train_data,\n",
    "#             history_len=history_len,\n",
    "#             col_names=[normalized_column_names[0]], \n",
    "#             device=device)\n",
    "\n",
    "#         train_data_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "#         # test data preprocessing\n",
    "#         test_data, _ = epa_taiwan_data_pipeline.min_max_df_norm(test_data)\n",
    "\n",
    "#         test_dataset = epa_taiwan_data_pipeline.AqiDataset(\n",
    "#             test_data,\n",
    "#             history_len=history_len,\n",
    "#             col_names=[normalized_column_names[0]], \n",
    "#             device=device)\n",
    "\n",
    "#         test_data_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "#         # model preparation\n",
    "#         model_name = f\"{epoch}E_{lookback}W_{batch_size}B\"\n",
    "#         model = lstnet_gokul.LSTNet(\n",
    "#             ar_window_size=lookback,\n",
    "#             num_features=1,\n",
    "#             recc1_out_channels=64,\n",
    "#             conv1_out_channels=32).to(device)\n",
    "\n",
    "#         criterion = nn.MSELoss()\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#         # Define your training loop\n",
    "#         epoch_avg_train_loss, epoch_avg_test_loss = engine.train(\n",
    "#             model=model,\n",
    "#             train_dataloader=train_data_loader,\n",
    "#             test_dataloader=test_data_loader,\n",
    "#             optimizer=optimizer,\n",
    "#             loss_fn=criterion,\n",
    "#             epochs=epoch,\n",
    "#             device=device,\n",
    "#         )\n",
    "\n",
    "#         # print(\"LSTNET model (learning_rate={:f}, batch_size={:f}):\".format(lr, batch_size))\n",
    "\n",
    "#         # Log hyperparameters\n",
    "#         mlflow.log_param(\"epoch\",epoch)\n",
    "#         mlflow.log_param(\"batch_size\",batch_size)\n",
    "#         mlflow.log_param(\"lookback\",lookback)\n",
    "\n",
    "#         # Log metrics during training\n",
    "#         mlflow.log_metrics(\n",
    "#             {\n",
    "#                 \"train_loss\": epoch_avg_train_loss[0], \n",
    "#                 \"test_loss\": epoch_avg_test_loss[0]\n",
    "#             },\n",
    "#             step=epoch\n",
    "#         )\n",
    "\n",
    "#         # # Log additional artifacts\n",
    "#         # mlflow.log_artifact(\"path/to/your/training_plots.png\")\n",
    "\n",
    "#         # # Model registry does not work with file store\n",
    "#         # if tracking_url_type_store != \"file\":\n",
    "#         #     # Register the model\n",
    "#         #     # There are other ways to use the Model Registry, which depends on the use case,\n",
    "#         #     # please refer to the doc for more information:\n",
    "#         #     # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "#         #     mlflow.pytorch.log_model(\n",
    "#         #         model, model_name, registered_model_name=model_name\n",
    "#         #     )\n",
    "#         # else:\n",
    "#         #     mlflow.pytorch.log_model(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the best model\n",
    "- Prepare the test data\n",
    "- Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
